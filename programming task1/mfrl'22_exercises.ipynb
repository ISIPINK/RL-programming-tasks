{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eczUqZWUmaQj",
        "outputId": "ef7e8fe7-2c15-4d01-8e3c-51cc322d679b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[toy_text] in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gym[toy_text]) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gym[toy_text]) (1.23.5)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp310-cp310-win_amd64.whl (4.8 MB)\n",
            "     ---------------------------------------- 4.8/4.8 MB 11.3 MB/s eta 0:00:00\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Toegang geweigerd: 'C:\\\\Users\\\\Admin\\\\anaconda3\\\\Lib\\\\site-packages\\\\~ygame\\\\base.cp310-win_amd64.pyd'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install gym[toy_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading DiscreteEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.26.2\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from pt1_plot import *\n",
        "\n",
        "print(gym.__version__)\n",
        "\n",
        "# We will load a DiscreteEnv and retrieve the probability and reward information\n",
        "env = gym.make(\"FrozenLake8x8-v1\", desc=None, map_name=None, is_slippery=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy computation\n",
        "env.P[state][action] = [(probability, nextstate, reward, done), ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We search for a stationary policy, as evaluation happens on $T=1000$\n",
        "where it is improbable that the MDP isn't done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 3), (9, 3), (10, 3), (11, 3), (12, 3), (13, 2), (14, 1), (15, 1), (16, 3), (17, 3), (18, 3), (19, 0), (20, 0), (21, 2), (22, 1), (23, 1), (24, 3), (25, 3), (26, 3), (27, 3), (28, 1), (29, 2), (30, 1), (31, 1), (32, 0), (33, 0), (34, 2), (35, 3), (36, 3), (37, 2), (38, 2), (39, 1), (40, 0), (41, 1), (42, 2), (43, 0), (44, 0), (45, 2), (46, 2), (47, 1), (48, 0), (49, 2), (50, 0), (51, 0), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 0), (59, 0), (60, 0), (61, 2), (62, 2), (63, 0)]\n",
            "0.9999999999995879\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78d1cfcb023149f8b51ced819d53bfd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=0, description='iterations', max=299), Output()), _dom_classes=('widget-â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import copy\n",
        "alpha= 1\n",
        "states = range(len(env.P)) \n",
        "\n",
        "# initializing v,q,policy\n",
        "v = {state:0 for state in states} \n",
        "policy = {state:0 for state in states} # assume that 0 is an action\n",
        "q = {state:{action:0 for action in range(len(env.P[state])) } for state in states}\n",
        "\n",
        "#these are for plotting\n",
        "vv = [] \n",
        "pp = []\n",
        "qq = []\n",
        "\n",
        "for i in range(300):\n",
        "    for state in states:\n",
        "        info = env.P[state][policy[state]]\n",
        "        # the MC version samples the b, in a simulation we can importance sample b[0]\n",
        "        # a similar thing can be done with the q function\n",
        "        v[state] = sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in info)\n",
        "\n",
        "    for state in range(len(env.P)):\n",
        "        for action in range(len(env.P[state])): \n",
        "            val = sum(b[0]*(b[2]+v[b[1]]) for b in env.P[state][action])\n",
        "            q[state][action]= val\n",
        "\n",
        "    for state in states:\n",
        "        ma , mval = 0, q[state][0]\n",
        "        for action,val in q[state].items():\n",
        "            if val>mval:\n",
        "                ma = action \n",
        "                mval = val\n",
        "        policy[state] = ma\n",
        "    vv.append(v.copy())\n",
        "    pp.append(policy.copy())\n",
        "    qq.append(copy.deepcopy(q))\n",
        "\n",
        "sol = list(pp[-1].items())\n",
        "print(sol)\n",
        "print(vv[-1][0])\n",
        "intvp(vv,pp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy evaluation: here's where YOU also code\n",
        "Insert here your code to evaluate\n",
        "the total expected rewards over the planning horizon T\n",
        "if one follows your policy. Do the same for a random policy (i.e. the\n",
        "sample policy given above). As a sanity check, your policy should get an\n",
        "expected reward of at least the one obtained by the random policy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "uQxta2TbmmbP",
        "outputId": "3ba0b728-d0b3-44aa-b7e9-4fc4f6487cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC_eval of random policy: 0.022000000000000013\n",
            "MC_eval of sol policy: 1.0000000000000007\n"
          ]
        }
      ],
      "source": [
        "T = 1000  # Given horizon\n",
        "random_policy = {t: {i: env.action_space.sample()\n",
        "              for i in range(env.observation_space.n)}\n",
        "          for t in range(T)}\n",
        "\n",
        "sol_policy ={t: {i:pp[-1][i] \n",
        "              for i in range(env.observation_space.n)}\n",
        "                for t in range(T)}\n",
        "\n",
        "def MC_eval(pol,nsim = 10**3):\n",
        "    # does MC estimation of the expected reward\n",
        "    tmp = 0\n",
        "    for _ in range(nsim):\n",
        "        stat = env.reset()\n",
        "        total_reward = 0\n",
        "        state = stat[0]\n",
        "        for i, t in enumerate(range(T)): #wut i = t ?\n",
        "            # env.render()\n",
        "            action = pol[t][state]\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            total_reward +=reward\n",
        "            # if the MDP is stuck, we end the simulation here\n",
        "            if done:\n",
        "                break\n",
        "        env.close()\n",
        "        tmp +=total_reward/nsim\n",
        "    return tmp\n",
        "\n",
        "print(f\"MC_eval of random policy: {MC_eval(random_policy)}\")\n",
        "print(f\"MC_eval of sol policy: {MC_eval(sol_policy)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

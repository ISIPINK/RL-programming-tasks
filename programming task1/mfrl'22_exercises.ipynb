{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eczUqZWUmaQj",
        "outputId": "ef7e8fe7-2c15-4d01-8e3c-51cc322d679b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[toy_text] in c:\\users\\isido\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\isido\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[toy_text]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\isido\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[toy_text]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\isido\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in c:\\users\\isido\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[toy_text]) (2.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gym[toy_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependencies\n",
        "These are my visualizing tools, originally I had them in a .py file and imported them but google colabs don't support that.\n",
        "So before doing anything this has to be run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interactive\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def visq(q):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "    # Find the minimum and maximum Q-values across all actions and states\n",
        "    min_q = min(q[state].get(action, 0)\n",
        "                for action in range(4) for state in range(64))\n",
        "    max_q = max(q[state].get(action, 0)\n",
        "                for action in range(4) for state in range(64))\n",
        "    for action in range(4):\n",
        "        # Convert 'q' for the specific action to a 2D numpy array\n",
        "        q_array = np.zeros((8, 8))\n",
        "        for state in range(64):\n",
        "            row, col = divmod(state, 8)\n",
        "            q_value = q[state].get(action, 0)\n",
        "            q_array[row][col] = q_value\n",
        "        # Create a heatmap for this action with a common color scale\n",
        "        ax = axes[action // 2, action % 2]\n",
        "        im = ax.imshow(q_array, cmap='hot',\n",
        "                       interpolation='nearest', vmin=min_q, vmax=max_q)\n",
        "        ax.set_title(f'Action {action}')\n",
        "        plt.colorbar(im, ax=ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def intq(qq):\n",
        "    def plot_qq(i):\n",
        "        visq(qq[i])\n",
        "\n",
        "    i_slider = widgets.IntSlider(\n",
        "        value=0, min=0, max=len(qq) - 1, description='iterations')\n",
        "    interactive_plot = interactive(plot_qq, i=i_slider)\n",
        "    display(interactive_plot)\n",
        "\n",
        "\n",
        "def visualize_value_and_policy(v, policy):\n",
        "    # Convert 'v' to a 2D numpy array\n",
        "    v_array = np.zeros((8, 8))\n",
        "    for state in v:\n",
        "        row, col = divmod(state, 8)\n",
        "        v_array[row][col] = v[state]\n",
        "\n",
        "    # Create a grid of coordinates for arrows\n",
        "    x, y = np.meshgrid(np.arange(8), np.arange(8))\n",
        "\n",
        "    # Create an array of action vectors based on the policy\n",
        "    dx = np.zeros_like(v_array, dtype=int)\n",
        "    dy = np.zeros_like(v_array, dtype=int)\n",
        "\n",
        "    for state in range(64):\n",
        "        row, col = divmod(state, 8)\n",
        "        action = policy[state]\n",
        "        if action == 0:  # Left\n",
        "            dx[row, col] = -1\n",
        "        elif action == 1:  # Down\n",
        "            dy[row, col] = 1\n",
        "        elif action == 2:  # Right\n",
        "            dx[row, col] = 1\n",
        "        elif action == 3:  # Up\n",
        "            dy[row, col] = -1\n",
        "\n",
        "    # Create a combined plot with value function and policy arrows\n",
        "    fig, ax = plt.subplots()\n",
        "    cax = ax.matshow(v_array, cmap='hot')\n",
        "    plt.colorbar(cax)\n",
        "    ax.quiver(x, y, dx, dy, angles='xy',\n",
        "              scale_units='xy', scale=3, color='blue')\n",
        "    ax.set_xlim(-1, 8)\n",
        "    ax.set_ylim(-1, 8)\n",
        "    plt.gca().invert_yaxis()\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def intvp(vv, pp):\n",
        "    def vvap(i):\n",
        "        visualize_value_and_policy(vv[i], pp[i])\n",
        "    i_slider = widgets.IntSlider(value=0, min=0, max=min(\n",
        "        len(vv), len(pp)) - 1, description='iterations')\n",
        "    interactive_plot = interactive(vvap, i=i_slider)\n",
        "    display(interactive_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading DiscreteEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.26.2\n"
          ]
        }
      ],
      "source": [
        "from pt1_plot import *\n",
        "\n",
        "print(gym.__version__)\n",
        "\n",
        "# We will load a DiscreteEnv and retrieve the probability and reward information\n",
        "env = gym.make(\"FrozenLake8x8-v1\", desc=None, map_name=None, is_slippery=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy computation\n",
        "env.P[state][action] = [(probability, nextstate, reward, done), ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We search for a stationary policy, as evaluation happens on $T=1000$\n",
        "where it is improbable that the MDP isn't done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0), (1, 0), (2, 2), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 0), (9, 1), (10, 0), (11, 0), (12, 2), (13, 2), (14, 2), (15, 2), (16, 0), (17, 0), (18, 2), (19, 1), (20, 3), (21, 2), (22, 2), (23, 2), (24, 1), (25, 1), (26, 1), (27, 0), (28, 0), (29, 2), (30, 3), (31, 2), (32, 3), (33, 3), (34, 3), (35, 3), (36, 1), (37, 0), (38, 0), (39, 2), (40, 3), (41, 0), (42, 3), (43, 3), (44, 3), (45, 3), (46, 1), (47, 2), (48, 3), (49, 0), (50, 0), (51, 0), (52, 0), (53, 0), (54, 2), (55, 2), (56, 2), (57, 0), (58, 0), (59, 1), (60, 2), (61, 0), (62, 0), (63, 0)]\n",
            "0.8958558845705116\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35707ea1a07e4e40b41427b51894fc5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=0, description='iterations', max=299), Output()), _dom_classes=('widget-â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpha= 1\n",
        "states = range(len(env.P)) \n",
        "\n",
        "# initializing v,q,policy\n",
        "v = {state:0 for state in states} \n",
        "policy = {state:0 for state in states} # assume that 0 is an action\n",
        "q = {state:{action:0 for action in range(len(env.P[state])) } for state in states}\n",
        "\n",
        "#these are for plotting\n",
        "vv = [] \n",
        "pp = []\n",
        "\n",
        "for i in range(300):\n",
        "    for state in states:\n",
        "        info = env.P[state][policy[state]]\n",
        "        # the MC version samples the b, in a simulation we can importance sample b[0]\n",
        "        # a similar thing can be done with the q function\n",
        "        v[state] = sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in info)\n",
        "\n",
        "    for state in range(len(env.P)):\n",
        "        for action in range(len(env.P[state])): \n",
        "            val = sum(b[0]*(b[2]+v[b[1]]) for b in env.P[state][action])\n",
        "            q[state][action]= val\n",
        "\n",
        "    for state in states:\n",
        "        ma , mval = 0, q[state][0]\n",
        "        for action,val in q[state].items():\n",
        "            if val>mval:\n",
        "                ma = action \n",
        "                mval = val\n",
        "        policy[state] = ma\n",
        "    vv.append(v.copy())\n",
        "    pp.append(policy.copy())\n",
        "\n",
        "sol = list(pp[-1].items())\n",
        "print(sol)\n",
        "print(vv[-1][0])\n",
        "intvp(vv,pp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy evaluation: here's where YOU also code\n",
        "Insert here your code to evaluate\n",
        "the total expected rewards over the planning horizon T\n",
        "if one follows your policy. Do the same for a random policy (i.e. the\n",
        "sample policy given above). As a sanity check, your policy should get an\n",
        "expected reward of at least the one obtained by the random policy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "uQxta2TbmmbP",
        "outputId": "3ba0b728-d0b3-44aa-b7e9-4fc4f6487cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC_eval of random policy: 0.0\n",
            "MC_eval of sol policy: 0.8990000000000007\n"
          ]
        }
      ],
      "source": [
        "T = 1000  # Given horizon\n",
        "random_policy = {t: {i: env.action_space.sample()\n",
        "              for i in range(env.observation_space.n)}\n",
        "          for t in range(T)}\n",
        "\n",
        "sol_policy ={t: {i:pp[-1][i] \n",
        "              for i in range(env.observation_space.n)}\n",
        "                for t in range(T)}\n",
        "\n",
        "def MC_eval(pol,nsim = 10**3):\n",
        "    # does MC estimation of the expected reward\n",
        "    tmp = 0\n",
        "    for _ in range(nsim):\n",
        "        stat = env.reset()\n",
        "        total_reward = 0\n",
        "        state = stat[0]\n",
        "        for i, t in enumerate(range(T)): #wut i = t ?\n",
        "            # env.render()\n",
        "            action = pol[t][state]\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            total_reward +=reward\n",
        "            # if the MDP is stuck, we end the simulation here\n",
        "            if done:\n",
        "                break\n",
        "        env.close()\n",
        "        tmp +=total_reward/nsim\n",
        "    return tmp\n",
        "\n",
        "print(f\"MC_eval of random policy: {MC_eval(random_policy)}\")\n",
        "print(f\"MC_eval of sol policy: {MC_eval(sol_policy)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

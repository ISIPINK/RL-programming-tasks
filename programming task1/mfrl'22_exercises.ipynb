{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eczUqZWUmaQj",
        "outputId": "ef7e8fe7-2c15-4d01-8e3c-51cc322d679b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[toy_text] in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gym[toy_text]) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gym[toy_text]) (1.23.5)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp310-cp310-win_amd64.whl (4.8 MB)\n",
            "     ---------------------------------------- 4.8/4.8 MB 11.3 MB/s eta 0:00:00\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Toegang geweigerd: 'C:\\\\Users\\\\Admin\\\\anaconda3\\\\Lib\\\\site-packages\\\\~ygame\\\\base.cp310-win_amd64.pyd'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install gym[toy_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "uQxta2TbmmbP",
        "outputId": "3ba0b728-d0b3-44aa-b7e9-4fc4f6487cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.26.2\n",
            "Action = 2\n",
            "Action = 0\n",
            "Action = 2\n",
            "Action = 2\n",
            "Action = 2\n",
            "Action = 0\n",
            "Action = 2\n",
            "Action = 0\n",
            "Action = 1\n",
            "Action = 3\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "print(gym.__version__)\n",
        "\n",
        "\"\"\"\n",
        "You will not be able to see the simulation in colab. You can do it\n",
        "locally by un-commenting the following 4 lines and env.render() in the\n",
        "last piece of the code\n",
        "\"\"\"\n",
        "import pygame\n",
        "pygame.init()\n",
        "pygame.display.init()\n",
        "pygame.display.list_modes()\n",
        "\n",
        "\n",
        "# We will load a DiscreteEnv and retrieve the probability and reward\n",
        "# information\n",
        "env = gym.make(\"FrozenLake8x8-v1\", desc=None, map_name=None, is_slippery=True)\n",
        "\"\"\"\n",
        "DiscreteEnv has an attribute P which holds everything we want as a\n",
        "dictionary of lists:\n",
        "P[s][a] = [(probability, nextstate, reward, done), ...]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# first, we initialize the structures with zeros\n",
        "prob = {i: {j: {a: 0 for a in range(env.action_space.n)}\n",
        "            for j in range(env.observation_space.n)}\n",
        "        for i in range(env.observation_space.n)}\n",
        "rewd = {i: {j: {a: 0 for a in range(env.action_space.n)}\n",
        "            for j in range(env.observation_space.n)}\n",
        "        for i in range(env.observation_space.n)}\n",
        "# then, we fill them with the actual information\n",
        "for i in range(env.observation_space.n):\n",
        "    for a in range(env.action_space.n):\n",
        "        for (p, j, r, d) in env.P[i][a]:\n",
        "            prob[i][j][a] += p\n",
        "            rewd[i][j][a] += r\n",
        "\n",
        "\n",
        "# Policy computation: here's where YOU code\n",
        "\"\"\"\n",
        "Insert your clever policy computation here! make sure to replace the\n",
        "policy dictionary below by the results of your computation\n",
        "\"\"\"\n",
        "T = 10  # Given horizon\n",
        "policy = {t: {i: env.action_space.sample()\n",
        "              for i in range(env.observation_space.n)}\n",
        "          for t in range(T)}\n",
        "\n",
        "\n",
        "\n",
        "# Policy evaluation: here's where YOU also code\n",
        "\"\"\"\n",
        "Insert here your code to evaluate\n",
        "the total expected rewards over the planning horizon T\n",
        "if one follows your policy. Do the same for a random policy (i.e. the\n",
        "sample policy given above). As a sanity check, your policy should get an\n",
        "expected reward of at least the one obtained by the random policy!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# # Simulation: you can try your policy here\n",
        "# state = env.reset()\n",
        "# for t in range(T):\n",
        "#     #env.render()\n",
        "#     action = policy[t][state]\n",
        "#     print(state)\n",
        "#     print(f\"Action = {action}\")\n",
        "#     state, reward, done, _ = env.step(action)\n",
        "#     # if the MDP is stuck, we end the simulation here\n",
        "#     if done:\n",
        "#         print(state)\n",
        "#         print(f\"Episode finished after {t + 1} timesteps\")\n",
        "#         break\n",
        "# env.close()\n",
        "\n",
        "\n",
        "# Simulation: you can try your policy here\n",
        "state = env.reset()\n",
        "for i, t in enumerate(range(T)):\n",
        "    env.render()\n",
        "    if i ==0:\n",
        "        action = policy[t][state[0]]\n",
        "        print(f\"Action = {action}\")\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        # print(state, reward, done)\n",
        "    else:\n",
        "        action = policy[t][state]\n",
        "        print(f\"Action = {action}\")\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        # print(state, reward, done)\n",
        "\n",
        "    # if the MDP is stuck, we end the simulation here\n",
        "    if done:\n",
        "        print(f\"Episode finished after {t + 1} timesteps\")\n",
        "        break\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks we are given the transition of the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fWaLJbObaBq",
        "outputId": "0ed7634b-7220-4f42-9972-4b5d5c1d870e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0.3333333333333333, 61, 0.0, False), (0.3333333333333333, 62, 0.0, False), (0.3333333333333333, 63, 1.0, True)]\n"
          ]
        }
      ],
      "source": [
        "state = 62\n",
        "action = 1\n",
        "print(env.P[state][action])\n",
        "#[(probability, nextstate, reward, done), ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we need to compute is an approximate optimal policy in every state.\n",
        "We "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = range(len(env.P)) # must be immutable\n",
        "v = {state:0 for state in states} \n",
        "policy = {state:0 for state in states} # assume that 0 is an action\n",
        "q = {state:{action:0 for action in range(len(env.P[state])) } for state in states}\n",
        "\n",
        "vv = []\n",
        "pp = []\n",
        "qq = []\n",
        "\n",
        "for i in range(1000):\n",
        "    for state in states:\n",
        "        info = env.P[state][policy[state]]\n",
        "        v[state] = sum(b[0]*b[2]+v[b[1]] for b in info)/len(info)\n",
        "\n",
        "    for state in range(len(env.P)):\n",
        "        for action in range(len(env.P[state])): \n",
        "            val = sum(b[0]*b[2]+v[b[1]] for b in env.P[state][action])/len(env.P[state][action])\n",
        "            q[state][action]= val\n",
        "\n",
        "    for state in states:\n",
        "        ma , mval = 0, q[state][0]\n",
        "        for action,val in q[state].items():\n",
        "            if val>mval:\n",
        "                ma = action \n",
        "                mval = val\n",
        "        policy[state] = ma\n",
        "    vv.append(v.copy())\n",
        "    pp.append(policy.copy())\n",
        "    qq.append(q.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e890fb56a26846f8ad74e7c6140a93ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=0, description='i', max=99), Output()), _dom_classes=('widget-interact',…"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interactive\n",
        "\n",
        "def plot_vv(i): \n",
        "    visv(vv[i])\n",
        "\n",
        "i_slider = widgets.IntSlider(value=0, min=0, max=len(vv) - 1, description='i')\n",
        "interactive_plot = interactive(plot_vv, i=i_slider)\n",
        "interactive_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#chatgpt wrote this\n",
        "def visq(q):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "    # Find the minimum and maximum Q-values across all actions and states\n",
        "    min_q = min(q[state].get(action, 0) for action in range(4) for state in range(64))\n",
        "    max_q = max(q[state].get(action, 0) for action in range(4) for state in range(64))\n",
        "    for action in range(4):\n",
        "        # Convert 'q' for the specific action to a 2D numpy array\n",
        "        q_array = np.zeros((8, 8))\n",
        "        for state in range(64):\n",
        "            row, col = divmod(state, 8)\n",
        "            q_value = q[state].get(action, 0)\n",
        "            q_array[row][col] = q_value\n",
        "        # Create a heatmap for this action with a common color scale\n",
        "        ax = axes[action // 2, action % 2]\n",
        "        im = ax.imshow(q_array, cmap='hot', interpolation='nearest', vmin=min_q, vmax=max_q)\n",
        "        ax.set_title(f'Action {action}')\n",
        "        plt.colorbar(im, ax=ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#chatgpt wrote this\n",
        "def visv(v):\n",
        "    # Convert 'v' to a 2D numpy array\n",
        "    v_array = np.zeros((8, 8))\n",
        "    for state in v:\n",
        "        row, col = divmod(state, 8)\n",
        "        v_array[row][col] = v[state]\n",
        "\n",
        "    # plt.imshow(v_array, cmap='hot', interpolation='nearest')\n",
        "    plt.imshow(v_array, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "#chatgpt wrote this\n",
        "def visp(policy):\n",
        "    # Convert 'v' to a 2D numpy array\n",
        "    # Convert 'policy' to a 2D numpy array\n",
        "    policy_array = np.zeros((8, 8))\n",
        "    for state in policy:\n",
        "        row, col = divmod(state, 8)\n",
        "        policy_array[row][col] = policy[state]\n",
        "\n",
        "    # Create a heatmap for the policy\n",
        "    plt.imshow(policy_array, cmap='tab10', interpolation='nearest', vmin=0, vmax=3)  # Assuming 0 to 3 are the action values\n",
        "    plt.colorbar(ticks=[0, 1, 2, 3])  # Set ticks to represent the action values\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

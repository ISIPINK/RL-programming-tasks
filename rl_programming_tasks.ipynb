{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from plt_utils import *\n",
    "from test_levels import *\n",
    "import gym\n",
    "print(gym.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy computation discounted \n",
    "- policy_update corresponds to $f_v$ <br>\n",
    "- value_update(iterations =1) corresponds to $L_{\\text{policy}}$ <br>\n",
    "- value_update_seidel(iterations =1) uses updated values of v \n",
    "as soon that they are updated like gauss-seidel method. <br>\n",
    "\n",
    "- value_iteration is applying $L_{f_v}v = Uv$ (theorem 1.3.5)  <br>\n",
    "- backward_induction is value_iteration with $v^{T+1} =0$ and time dependence (theorem 1.2.1) <br>\n",
    "- policy_iteration is generalized_iteration because of (theorem 1.3.6) <br>\n",
    "we don't apply $L_{\\pi}$ infinite times but to convergence or $1000$ \n",
    "iterations.\n",
    "\n",
    "the code is obvious, there is a small difference in the formula <br> \n",
    "because rewards are defined differently so you take it in the sum <br>\n",
    "that way you get averaged reward corresponding to our definition <br>\n",
    "and you can decode $b$ with: <br>\n",
    "env.P[state][action] = [(probability, nextstate, reward, done), ...] <br>\n",
    "(To make backward induction time dependent just make env.P[state][action] \n",
    "time dependent.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states(env): return range(env.observation_space.n)\n",
    "def actions(env, state): return env.P[state].keys()\n",
    "# infmetric, norm, only works for v\n",
    "def oometric(v1, v2): return max(abs(v1[i]-v2[i]) for i in v1.keys())\n",
    "\n",
    "def value_update(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*vold[b[1]]) \n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "def value_update_seidel(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*v[b[1]]) # v instead vold, an OG error  for finite problem\n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "\n",
    "def policy_update(env, alpha, v, policy):\n",
    "    for state in states(env):\n",
    "        max_a, max_val = 0, -float(\"inf\") #to select the argmax\n",
    "        for action in actions(env, state):\n",
    "            val = sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in env.P[state][action])\n",
    "            max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "        policy[state] = max_a\n",
    "\n",
    "def value_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        value_update(env,alpha,v,policy,iterations=1)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and oometric(vv[-1], vv[-2]) < eps: break\n",
    "    return vv,pp\n",
    "\n",
    "def backward_induction(env,alpha,T):\n",
    "    return value_iteration(env,alpha,T,0)\n",
    "\n",
    "def generalized_iteration(env,alpha,inner_iter=1,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        value_update(env,alpha,v,policy,inner_iter,eps)\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and pp[-1]==pp[-2]==pp[-3]: break\n",
    "    return vv,pp\n",
    "\n",
    "def policy_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    return generalized_iteration(env=env,alpha=alpha,inner_iter=10**3,max_iter = max_iter,eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 3), (2, 3), (3, 3), (4, 3), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 1), (16, 0), (17, 0), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 0), (25, 0), (26, 2), (27, 2), (28, 2), (29, 2), (30, 1), (31, 1), (32, 0), (33, 0), (34, 2), (35, 2), (36, 3), (37, 2), (38, 1), (39, 1), (40, 0), (41, 0), (42, 2), (43, 0), (44, 0), (45, 2), (46, 1), (47, 1), (48, 0), (49, 0), (50, 2), (51, 0), (52, 0), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 0), (60, 0), (61, 2), (62, 2), (63, 0)]\n",
      "val(0) = 0.8334426747830409 at time 1 for finite horizon\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "# env=gym.make('CliffWalking-v0')\n",
    "# env= gym.make('Taxi-v3') # use value iteration\n",
    "vv,pp = backward_induction(env,alpha=1,T=1000) #programming task 1\n",
    "# vv,pp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) #programming task 2\n",
    "# vv,pp = value_iteration(env,alpha=0.999,max_iter=300,eps = 0.001) #programming task 3\n",
    "sol = list(pp[-1].items()) # asked form of the policy in class\n",
    "print(sol)\n",
    "print(f\"val(0) = {vv[-1][0]} at time 1 for finite horizon\")\n",
    "# intvp(vv,pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation discounted\n",
    "- value_eval is for evaluating a policy, basically it is a big  <br>\n",
    "average of the rewards times the probability of getting it <br>\n",
    "lot of terms can be reused, value_update does it basically  <br>\n",
    "- MC_eval just is MC estimation of the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_value_eval(env,alpha,pp):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    vv= []\n",
    "    for p in pp:\n",
    "        value_update(env,alpha,v,p,1,0)\n",
    "        vv.append(v.copy())\n",
    "    print(f\"finite val(0) = {vv[-1][0]}\")\n",
    "    return vv\n",
    "\n",
    "def infinite_value_eval(env,alpha,policy,eps=10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    value_update(env,alpha,v,policy,10**5,eps)\n",
    "    print(f\"infinite val(0) = {v[0]}\")\n",
    "    return v \n",
    "\n",
    "# does MC estimation of the expected reward\n",
    "def MC_eval(env,pol,alpha,T=10**3,nsim = 10**3):\n",
    "    running_sum = 0\n",
    "    for _ in range(nsim):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        for t in range(T): \n",
    "            action = pol[t][state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += alpha**t*reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "        running_sum +=total_reward/nsim\n",
    "    print(f\"infinite MC val(0) = {running_sum}\")\n",
    "    return running_sum\n",
    "\n",
    "def random_policy(env,T):\n",
    "    return [{state:env.action_space.sample() \n",
    "            for state in states(env)}\n",
    "            for _ in range(T)]\n",
    "\n",
    "def infinite_pol_rep(p,T):\n",
    "    return {t: {i:p[i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}\n",
    "\n",
    "def finite_pol_rep(pp,T):\n",
    "    return {t: {i:pp[-t-1][i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.7076089300881696\n",
      "infinite MC val(0) = 0.7027054062044972\n",
      "reference = 0.7074435915028664\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "alpha = 0.999\n",
    "T = 10**3\n",
    "pvv,ppp = value_iteration(env,alpha=alpha,max_iter=600,eps=10**(-6)) \n",
    "infinite_value_eval(env,alpha,ppp[-1])\n",
    "MC_eval(env,infinite_pol_rep(ppp[-1],T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {pvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 0.011042662185476242\n",
      "infinite MC val(0) = 0.004881404151425044\n",
      "reference = 0.011042662185476242\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "alpha=0.999\n",
    "T = 30\n",
    "bvv,bpp = backward_induction(env,alpha=alpha,T=T) \n",
    "finite_value_eval(env,alpha=alpha,pp=bpp)\n",
    "MC_eval(env,finite_pol_rep(bpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {bvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 6.326043740530512e-05\n",
      "infinite MC val(0) = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# env = trivial()\n",
    "env = time_level()\n",
    "alpha=0.999\n",
    "T = 400\n",
    "rpp = random_policy(env,T)\n",
    "finite_value_eval(env,alpha,rpp)\n",
    "MC_eval(env,finite_pol_rep(rpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy evaluation average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conjecture that:\n",
    "$$\n",
    "P^*(f) = \\lim_{n \\to \\infty} \\frac{1}{l} \\sum_{k=0}^{l-1} P^{k+n}(f).\n",
    "$$ \n",
    "with $l$ is  the least common multiple of the periods of irreducible communication classes of P(f).\n",
    "In this problem $l=1$ because the only irreducible classes possibles are the holes, the goal and \n",
    "one that contain \"wall turn\" backs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_array,eye\n",
    "from scipy.sparse.linalg import norm \n",
    "\n",
    "def P_from_policy(env,policy):\n",
    "    n = len(states(env))\n",
    "    P = dok_array((n, n), dtype=np.float32)\n",
    "    for i in states(env):\n",
    "        for info in env.P[i][policy[i]]:\n",
    "            P[i,info[1]] += info[0] \n",
    "    return P\n",
    "\n",
    "def power(P,n):\n",
    "   if n%2==0 and n!=0:\n",
    "        Q = power(P,n/2)\n",
    "        return Q@Q \n",
    "   return power(P,n-1)@P if n!=0 else eye(P.shape[0])\n",
    "\n",
    "def power_convergence(P,max_iter,eps =10**(-12)):\n",
    "    Q=P\n",
    "    for _ in range(max_iter):\n",
    "        Qnew = Q@Q\n",
    "        Qnew1 = P@Qnew\n",
    "        if norm(Q-Qnew)<eps and norm(Qnew-Qnew1):break\n",
    "        Q = Qnew1\n",
    "    return Q \n",
    "\n",
    "def stationary_matrix(P,lcm_periods, eps = 10**(-10)):\n",
    "    S = power_convergence(P,15,eps)\n",
    "    S.data[S.data < eps] = 0\n",
    "    S.eliminate_zeros()\n",
    "    return sum(S@power(P,j) for j in range(lcm_periods))/lcm_periods  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee11588ce2a741f5b769964dbaa27ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=0), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = envrandom()\n",
    "# pvv,ppp = policy_iteration(env,alpha=1) \n",
    "ppp = random_policy(env,1)\n",
    "pvv = [infinite_value_eval(env,1,ppp[-1])]\n",
    "P = P_from_policy(env,ppp[-1])\n",
    "intPP([P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd87555fa09e4671af5f8db63694b680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=0), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B = stationary_matrix(P,1)\n",
    "intPP([B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the reward vector is $1$ at the goal, the average reward must correspond to \n",
    "the discounted reward with $\\alpha =1$ which in this case can be calculated with \n",
    "the discounted tools. Here we check that they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between f and pvv[-1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab154a7dc964526bf4bdf71a2f0b699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=0), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q = stationary_matrix(P,1) \n",
    "r = np.array([1 if i==63 else 0 for i in range(64)])\n",
    "f = Q@r\n",
    "df ={state:f[state]-pvv[-1][state] for state in states(env)}\n",
    "print(\"difference between f and pvv[-1]\")\n",
    "intvp([df],[ppp[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy computation average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\varepsilon$-greedy Q-learning\n",
    "We assume we know the states and the actions ...,\n",
    "we start of from a value function because otherwise \n",
    "sparse rewards are a problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def get_action_from_q(q,state):\n",
    "    i = randint(0,3)\n",
    "    max_a , max_val = i, q[state][i]\n",
    "    for action,val in q[state].items():\n",
    "        max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "    return max_a,max_val    \n",
    "\n",
    "def get_q_from_v(env,v,alpha):\n",
    "    q = {state:{action:0 for action in actions(env,state) } for state in states(env)}\n",
    "    for state in states(env):\n",
    "        for action in actions(env,state): \n",
    "            q[state][action]= sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in env.P[state][action]) # uses P\n",
    "    return q\n",
    "\n",
    "def get_policy_from_q(env,q): # Programming task 5 (B)\n",
    "    return {state:get_action_from_q(q,state)[0] for state in states(env)}\n",
    "from random import randint\n",
    "def get_action_from_q(q,state):\n",
    "    i = randint(0,3)\n",
    "    max_a , max_val = i, q[state][i]\n",
    "    for action,val in q[state].items():\n",
    "        max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "    return max_a,max_val\n",
    "\n",
    "def get_q_from_v(env,v,alpha):\n",
    "    return {state:{action:\n",
    "      sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2]\n",
    "          for b in env.P[state][action]) for action in actions(env,state) } for state in states(env)}\n",
    "\n",
    "def get_policy_from_q(env,q): # Programming task 5 (B)\n",
    "    return {state:get_action_from_q(q,state)[0] for state in states(env)}\n",
    "\n",
    "def val_est(state):\n",
    "    row,col = divmod(state,8)\n",
    "    return (row+col)/14\n",
    "\n",
    "def q_0(env):\n",
    "    return {state:{action:0 for action in actions(env,state) } for state in states(env)}\n",
    "\n",
    "def q_est(env):\n",
    "    return {state:{action:val_est(state) for action in actions(env,state) } for state in states(env)}\n",
    "\n",
    "def Q_learning(env,v,alpha,gamma,T=10**3,nsim = 10**3):\n",
    "    goal_count = 0\n",
    "    q = get_q_from_v(env,v,alpha) # uses P\n",
    "    # q = q_est(env)\n",
    "    # q = q_0(env)\n",
    "    for i in range(nsim):\n",
    "        state = env.reset()[0] \n",
    "        for t in range(T):\n",
    "            action, _ = get_action_from_q(q,state) if randint(0,30)!=0 else (randint(0,3),1)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            _, qval2 = get_action_from_q(q,state)\n",
    "            q[state][action] = (1-gamma)*(q[state][action])+ gamma*(reward+alpha*qval2)\n",
    "            goal_count +=reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "    print(f\"reached goal {goal_count} times in sims\")\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.00015103296390637152\n",
      "reached goal 853.0 times in sims\n",
      "infinite val(0) = 0.0\n",
      "This interactive plot gives improvement of the value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed940bcd277e431fb28674de6298cb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=0), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = time_level()\n",
    "env = envrandom()\n",
    "alpha = 1\n",
    "rp = random_policy(env,1)[0]\n",
    "rv = infinite_value_eval(env,alpha,rp) #uses P\n",
    "q= Q_learning(env,rv,alpha=alpha,gamma=0.2,T=2*10**2,nsim=10**4) # Programming task 5\n",
    "qp = get_policy_from_q(env,q)\n",
    "\n",
    "# Programming task 5 (C) and (D)\n",
    "qv = infinite_value_eval(env,alpha,qp)\n",
    "dv = {state:(qv[state]-rv[state]) for state in states(env)}\n",
    "# intvp([rv], [rp])\n",
    "print(\"This interactive plot gives improvement of the value\")\n",
    "intvp([dv], [qp])\n",
    "# visq(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 ms ± 191 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "B = stationary_matrix(P,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between f and pvv[-1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d00b2d178f546959e859fc83d262ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=0), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

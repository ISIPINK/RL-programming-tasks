{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "from plt_utils import *\n",
    "from test_levels import *\n",
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy computation\n",
    "- policy_update corresponds to $f_v$ <br>\n",
    "- value_update(iterations =1) corresponds to $L_{\\text{policy}}$ <br>\n",
    "- value_update_seidel(iterations =1) uses updated values of v \n",
    "as soon that they are updated like gauss-seidel method. <br>\n",
    "\n",
    "- value_iteration is applying $L_{f_v}v = Uv$ (theorem 1.3.5)  <br>\n",
    "- backward_induction is value_iteration with $v^{T+1} =0$ and time dependence (theorem 1.2.1) <br>\n",
    "- policy_iteration is generalized_iteration because of (theorem 1.3.6) <br>\n",
    "we don't apply $L_{\\pi}$ infinite times but to convergence or $1000$ \n",
    "iterations.\n",
    "\n",
    "the code is obvious, there is a small difference in the formula <br> \n",
    "because rewards are defined differently so you take it in the sum <br>\n",
    "that way you get averaged reward corresponding to our definition <br>\n",
    "and you can decode $b$ with: <br>\n",
    "env.P[state][action] = [(probability, nextstate, reward, done), ...] <br>\n",
    "(To make backward induction time dependent just make env.P[state][action] \n",
    "time dependent.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states(env): return range(env.observation_space.n)\n",
    "def actions(env, state): return env.P[state].keys()\n",
    "# infmetric, norm, only works for v\n",
    "def oometric(v1, v2): return max(abs(v1[i]-v2[i]) for i in v1.keys())\n",
    "\n",
    "def value_update(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*vold[b[1]]) \n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "def value_update_seidel(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*v[b[1]]) # v instead vold, an OG error  for finite problem\n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "\n",
    "def policy_update(env, alpha, v, policy):\n",
    "    for state in states(env):\n",
    "        max_a, max_val = 0, -float(\"inf\") #to select the argmax\n",
    "        for action in actions(env, state):\n",
    "            val = sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in env.P[state][action])\n",
    "            max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "        policy[state] = max_a\n",
    "\n",
    "def value_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        value_update(env,alpha,v,policy,iterations=1)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and oometric(vv[-1], vv[-2]) < eps: break\n",
    "    return vv,pp\n",
    "\n",
    "def backward_induction(env,alpha,T):\n",
    "    return value_iteration(env,alpha,T,0)\n",
    "\n",
    "def generalized_iteration(env,alpha,inner_iter=1,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        value_update(env,alpha,v,policy,inner_iter,eps)\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and pp[-1]==pp[-2]==pp[-3]: break\n",
    "    return vv,pp\n",
    "\n",
    "def policy_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    return generalized_iteration(env=env,alpha=alpha,inner_iter=10**3,max_iter = max_iter,eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (1, 4), (2, 4), (3, 4), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 5), (17, 0), (18, 0), (19, 0), (20, 3), (21, 3), (22, 3), (23, 3), (24, 0), (25, 0), (26, 0), (27, 0), (28, 0), (29, 0), (30, 0), (31, 0), (32, 0), (33, 0), (34, 0), (35, 0), (36, 3), (37, 0), (38, 0), (39, 0), (40, 0), (41, 0), (42, 0), (43, 0), (44, 2), (45, 2), (46, 2), (47, 2), (48, 0), (49, 0), (50, 0), (51, 0), (52, 0), (53, 0), (54, 0), (55, 0), (56, 0), (57, 2), (58, 0), (59, 0), (60, 0), (61, 0), (62, 0), (63, 0), (64, 2), (65, 2), (66, 2), (67, 2), (68, 0), (69, 0), (70, 0), (71, 0), (72, 0), (73, 0), (74, 0), (75, 0), (76, 0), (77, 2), (78, 0), (79, 0), (80, 0), (81, 0), (82, 0), (83, 0), (84, 4), (85, 4), (86, 4), (87, 4), (88, 0), (89, 0), (90, 0), (91, 0), (92, 0), (93, 0), (94, 0), (95, 0), (96, 0), (97, 5), (98, 0), (99, 0), (100, 1), (101, 1), (102, 1), (103, 1), (104, 0), (105, 0), (106, 0), (107, 0), (108, 0), (109, 0), (110, 0), (111, 0), (112, 0), (113, 0), (114, 0), (115, 0), (116, 1), (117, 0), (118, 0), (119, 0), (120, 1), (121, 1), (122, 1), (123, 1), (124, 0), (125, 0), (126, 0), (127, 0), (128, 0), (129, 0), (130, 0), (131, 0), (132, 0), (133, 0), (134, 0), (135, 0), (136, 1), (137, 0), (138, 0), (139, 0), (140, 0), (141, 0), (142, 0), (143, 0), (144, 1), (145, 1), (146, 1), (147, 1), (148, 0), (149, 0), (150, 0), (151, 0), (152, 0), (153, 0), (154, 0), (155, 0), (156, 0), (157, 1), (158, 0), (159, 0), (160, 0), (161, 0), (162, 0), (163, 0), (164, 1), (165, 1), (166, 1), (167, 1), (168, 0), (169, 0), (170, 0), (171, 0), (172, 0), (173, 0), (174, 0), (175, 0), (176, 0), (177, 1), (178, 0), (179, 0), (180, 0), (181, 0), (182, 0), (183, 0), (184, 1), (185, 1), (186, 1), (187, 1), (188, 0), (189, 0), (190, 0), (191, 0), (192, 0), (193, 0), (194, 0), (195, 0), (196, 0), (197, 1), (198, 0), (199, 0), (200, 1), (201, 1), (202, 1), (203, 1), (204, 2), (205, 2), (206, 2), (207, 2), (208, 0), (209, 0), (210, 0), (211, 0), (212, 2), (213, 2), (214, 2), (215, 2), (216, 1), (217, 2), (218, 0), (219, 2), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (225, 2), (226, 2), (227, 2), (228, 3), (229, 3), (230, 3), (231, 3), (232, 2), (233, 2), (234, 2), (235, 2), (236, 1), (237, 2), (238, 3), (239, 2), (240, 3), (241, 3), (242, 3), (243, 3), (244, 1), (245, 1), (246, 1), (247, 1), (248, 3), (249, 3), (250, 3), (251, 3), (252, 2), (253, 2), (254, 2), (255, 2), (256, 3), (257, 1), (258, 3), (259, 2), (260, 3), (261, 3), (262, 3), (263, 3), (264, 1), (265, 1), (266, 1), (267, 1), (268, 3), (269, 3), (270, 3), (271, 3), (272, 0), (273, 0), (274, 0), (275, 0), (276, 3), (277, 1), (278, 3), (279, 0), (280, 3), (281, 3), (282, 3), (283, 3), (284, 1), (285, 1), (286, 1), (287, 1), (288, 3), (289, 3), (290, 3), (291, 3), (292, 0), (293, 0), (294, 0), (295, 0), (296, 3), (297, 1), (298, 3), (299, 0), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 0), (309, 0), (310, 0), (311, 0), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 0), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1), (349, 1), (350, 1), (351, 1), (352, 1), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 1), (370, 1), (371, 1), (372, 0), (373, 0), (374, 0), (375, 0), (376, 1), (377, 1), (378, 1), (379, 0), (380, 1), (381, 1), (382, 1), (383, 1), (384, 1), (385, 1), (386, 1), (387, 1), (388, 1), (389, 1), (390, 1), (391, 1), (392, 0), (393, 0), (394, 0), (395, 0), (396, 1), (397, 1), (398, 1), (399, 0), (400, 1), (401, 1), (402, 1), (403, 1), (404, 1), (405, 1), (406, 1), (407, 1), (408, 4), (409, 4), (410, 4), (411, 4), (412, 1), (413, 1), (414, 1), (415, 1), (416, 1), (417, 1), (418, 5), (419, 1), (420, 1), (421, 1), (422, 1), (423, 1), (424, 1), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 1), (431, 1), (432, 1), (433, 1), (434, 1), (435, 1), (436, 1), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 1), (443, 1), (444, 1), (445, 1), (446, 1), (447, 1), (448, 1), (449, 1), (450, 1), (451, 1), (452, 1), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (462, 1), (463, 1), (464, 1), (465, 1), (466, 1), (467, 1), (468, 1), (469, 1), (470, 1), (471, 1), (472, 4), (473, 4), (474, 4), (475, 4), (476, 1), (477, 1), (478, 1), (479, 5), (480, 1), (481, 1), (482, 1), (483, 1), (484, 1), (485, 1), (486, 1), (487, 1), (488, 1), (489, 1), (490, 1), (491, 1), (492, 3), (493, 3), (494, 3), (495, 3), (496, 1), (497, 1), (498, 1), (499, 3)]\n",
      "val(0) = 18.98 at time 1 for finite horizon\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "# env=gym.make('CliffWalking-v0')\n",
    "# env= gym.make('Taxi-v3') # use value iteration\n",
    "# vv,pp = backward_induction(env,alpha=1,T=1000) #programming task 1\n",
    "# vv,pp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) #programming task 2\n",
    "vv,pp = value_iteration(env,alpha=0.999,max_iter=300,eps = 0.001) #programming task 3\n",
    "sol = list(pp[-1].items()) # asked form of the policy in class\n",
    "print(sol)\n",
    "print(f\"val(0) = {vv[-1][0]} at time 1 for finite horizon\")\n",
    "# intvp(vv,pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation\n",
    "- value_eval is for evaluating a policy, basically it is a big  <br>\n",
    "average of the rewards times the probability of getting it <br>\n",
    "lot of terms can be reused, value_update does it basically  <br>\n",
    "- MC_eval just is MC estimation of the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_value_eval(env,alpha,pp):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    vv= []\n",
    "    for p in pp:\n",
    "        value_update(env,alpha,v,p,1,0)\n",
    "        vv.append(v.copy())\n",
    "    print(f\"finite val(0) = {vv[-1][0]}\")\n",
    "    return vv\n",
    "\n",
    "def infinite_value_eval(env,alpha,policy,eps=10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    value_update(env,alpha,v,policy,10**5,eps)\n",
    "    print(f\"infinite val(0) = {v[0]}\")\n",
    "    return v \n",
    "\n",
    "# does MC estimation of the expected reward\n",
    "def MC_eval(env,pol,alpha,T=10**3,nsim = 10**3):\n",
    "    running_sum = 0\n",
    "    for _ in range(nsim):\n",
    "        stat = env.reset()\n",
    "        # state = env.reset() #google colab\n",
    "        total_reward = 0\n",
    "        state = stat[0]\n",
    "        for t in range(T): \n",
    "            action = pol[t][state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += alpha**t*reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "        running_sum +=total_reward/nsim\n",
    "    print(f\"infinite MC val(0) = {running_sum}\")\n",
    "    return running_sum\n",
    "\n",
    "def random_policy(env,T):\n",
    "    return [{state:env.action_space.sample() \n",
    "            for state in states(env)}\n",
    "            for _ in range(T)]\n",
    "\n",
    "def infinite_pol_rep(p,T):\n",
    "    return {t: {i:p[i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}\n",
    "\n",
    "def finite_pol_rep(pp,T):\n",
    "    return {t: {i:pp[-t-1][i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.5164902723354029\n",
      "infinite MC val(0) = 0.5050410388335955\n",
      "reference = 0.5164920595206823\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "alpha = 0.999\n",
    "T = 10**3\n",
    "pvv,ppp = policy_iteration(env,alpha=alpha,max_iter=50,eps=10**(-6)) \n",
    "infinite_value_eval(env,alpha,ppp[-1])\n",
    "MC_eval(env,infinite_pol_rep(ppp[-1],T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {pvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 0.010602851206551666\n",
      "infinite MC val(0) = 0.008758193928672796\n",
      "reference = 0.010602851206551666\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "alpha=0.999\n",
    "T = 30\n",
    "bvv,bpp = backward_induction(env,alpha=alpha,T=T) \n",
    "finite_value_eval(env,alpha=alpha,pp=bpp)\n",
    "MC_eval(env,finite_pol_rep(bpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {bvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 0.35558098294602347\n",
      "infinite MC val(0) = 0.39182276166057983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = trivial()\n",
    "alpha=0.999\n",
    "T = 200\n",
    "rpp = random_policy(env,T)\n",
    "finite_value_eval(env,alpha,rpp)\n",
    "MC_eval(env,finite_pol_rep(rpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-greedy Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume we know the states and the actions ...,\n",
    "we start of from a value function because otherwise \n",
    "sparse rewards are a problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def get_action_from_q(q,state):\n",
    "    i = randint(0,3)\n",
    "    max_a , max_val = i, q[state][i]\n",
    "    for action,val in q[state].items():\n",
    "        max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "    return max_a,max_val    \n",
    "\n",
    "def get_q_from_v(env,v,alpha):\n",
    "    q = {state:{action:0 for action in actions(env,state) } for state in states(env)}\n",
    "    for state in states(env):\n",
    "        for action in actions(env,state): \n",
    "            q[state][action]= sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in env.P[state][action]) # uses P\n",
    "    return q\n",
    "\n",
    "def get_policy_from_q(env,q): # Programming task 5 (B)\n",
    "    return {state:get_action_from_q(q,state)[0] for state in states(env)}\n",
    "\n",
    "def Q_learning(env,v,alpha,gamma,T=10**3,nsim = 10**3):\n",
    "    q = get_q_from_v(env,v,alpha) # uses P\n",
    "    for i in range(nsim):\n",
    "        stat = env.reset()\n",
    "        # state = env.reset() #google colab\n",
    "        state = stat[0]\n",
    "        for t in range(T): \n",
    "            action, _ = get_action_from_q(q,state) if randint(0,10)!=0 else (randint(0,3),1)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            _, qval2 = get_action_from_q(q,state)\n",
    "            q[state][action] = (1-gamma)*(q[state][action])+ gamma*(reward+alpha*qval2)\n",
    "            if done: break\n",
    "        env.close()\n",
    "    return q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.0\n",
      "infinite val(0) = 0.2162460422496738\n",
      "This interactive plot gives improvement of the value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e7a5713ba147c0ae5ba88745d8b5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=0), IntSlider(value=0, description='rowâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = time_level()\n",
    "env = envrandom()\n",
    "alpha = 1\n",
    "rp = random_policy(env,1)[0]\n",
    "rv = infinite_value_eval(env,alpha,rp) #uses P\n",
    "q= Q_learning(env,rv,alpha=alpha,gamma=0.2,T=2*10**2,nsim=10**3) # Programming task 5\n",
    "qp = get_policy_from_q(env,q)\n",
    "\n",
    "# Programming task 5 (C) and (D)\n",
    "qv = infinite_value_eval(env,alpha,qp)\n",
    "dv = {state:(qv[state]-rv[state]) for state in states(env)}\n",
    "# intvp([rv], [rp])\n",
    "print(\"This interactive plot gives improvement of the value\")\n",
    "intvp([dv], [qp])\n",
    "# visq(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

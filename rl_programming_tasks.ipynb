{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "from plt_utils import *\n",
    "from test_levels import *\n",
    "import gym\n",
    "\n",
    "print(gym.__version__)\n",
    "env = time_level()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy computation\n",
    "- policy_update corresponds to $f_v$ <br>\n",
    "- value_update(iterations =1) corresponds to $L_{\\text{policy}}$ <br>\n",
    "\n",
    "- value_iteration is applying $L_{f_v}v = Uv$ (theorem 1.3.5)  <br>\n",
    "- backward_induction is value_iteration with $v^{T+1} =0$ and time dependence (theorem 1.2.1) <br>\n",
    "- policy_iteration is generalized_iteration because of (theorem 1.3.6) <br>\n",
    "we don't apply $L_{\\pi}$ infinite times but to convergence or $1000$ \n",
    "iterations.\n",
    "\n",
    "the code is obvious, there is a small difference in the formula <br> \n",
    "because rewards are defined differently so you take it in the sum <br>\n",
    "that way you get averaged reward corresponding to our definition <br>\n",
    "and you can decode $b$ with: <br>\n",
    "env.P[state][action] = [(probability, nextstate, reward, done), ...] <br>\n",
    "(To make backward induction time dependent just make env.P[state][action] \n",
    "time dependent.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states(env): return range(env.observation_space.n)\n",
    "def actions(env, state): return env.P[state].keys()\n",
    "# infmetric, norm, only works for v\n",
    "def oometric(v1, v2): return max(abs(v1[i]-v2[i]) for i in v1.keys())\n",
    "\n",
    "def value_update(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*v[b[1]]) \n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "\n",
    "\n",
    "def policy_update(env, alpha, v, policy):\n",
    "    for state in states(env):\n",
    "        max_a, max_val = 0, -float(\"inf\") #to select the argmax\n",
    "        for action in actions(env, state):\n",
    "            val = sum(b[0]*(b[2]+alpha*v[b[1]]) for b in env.P[state][action])\n",
    "            max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "        policy[state] = max_a\n",
    "\n",
    "def value_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        value_update(env,alpha,v,policy,iterations=1)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and oometric(vv[-1], vv[-2]) < eps: break\n",
    "    return vv,pp\n",
    "\n",
    "def backward_induction(env,alpha,T):\n",
    "    return value_iteration(env,alpha,T,0)\n",
    "\n",
    "def generalized_iteration(env,alpha,inner_iter=1,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        value_update(env,alpha,v,policy,inner_iter,eps)\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and pp[-1]==pp[-2]==pp[-3]: break\n",
    "    return vv,pp\n",
    "\n",
    "def policy_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    return generalized_iteration(env=env,alpha=alpha,inner_iter=10**3,max_iter = max_iter,eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 3), (2, 3), (3, 3), (4, 3), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 1), (16, 0), (17, 0), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 0), (25, 0), (26, 2), (27, 2), (28, 3), (29, 2), (30, 1), (31, 1), (32, 0), (33, 0), (34, 2), (35, 0), (36, 0), (37, 2), (38, 1), (39, 1), (40, 0), (41, 0), (42, 2), (43, 0), (44, 0), (45, 2), (46, 1), (47, 1), (48, 0), (49, 0), (50, 2), (51, 0), (52, 0), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 2), (59, 0), (60, 0), (61, 2), (62, 2), (63, 0)]\n",
      "val(0) = 0.5165094882368573 at time 1 for finite horizon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e458613d3e24a6da14f189612ae5682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=9), IntSlider(value=0, description='rowâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vv,pp = backward_induction(env,alpha=1,T=30) \n",
    "# vv,pp = backward_induction(env,alpha=1,T=1000) #programming task 1\n",
    "vv,pp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) #programming task 2\n",
    "# vv,pp = value_iteration(env,alpha=0.999,max_iter=300,eps = 0.001) #programming task 3\n",
    "sol = list(pp[-1].items()) # asked form of the policy\n",
    "print(sol)\n",
    "print(f\"val(0) = {vv[-1][0]} at time 1 for finite horizon\")\n",
    "intvp(vv,pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation\n",
    "value_eval is for evaluating a policy, basically it is a big  <br>\n",
    "average of the rewards times the probability of getting it <br>\n",
    "lot of terms can be reused, value_update does it basically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_value_eval(env,alpha,pp):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    vv= []\n",
    "    for p in pp:\n",
    "        value_update(env,alpha,v,p,1,0)\n",
    "        vv.append(v.copy())\n",
    "    print(f\"val(0) = {vv[-1][0]} at time 1 for finite horizon\")\n",
    "    return vv\n",
    "\n",
    "def infinite_value_eval(env,alpha,policy,eps=10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    value_update(env,alpha,v,policy,10**5,eps)\n",
    "    print(f\"val(0) = {v[0]} for infinite horizon\")\n",
    "    return v \n",
    "\n",
    "def random_policy(env,T):\n",
    "    return [{state:env.action_space.sample() \n",
    "            for state in states(env)}\n",
    "            for _ in range(T)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val(0) = 0.5165083584549253 for infinite horizon\n",
      "0.5165094882368573\n"
     ]
    }
   ],
   "source": [
    "pvv,ppp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) \n",
    "pv = infinite_value_eval(env,0.999,ppp[-1])\n",
    "print(pvv[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val(0) = 0.02869336917456404 at time 1 for finite horizon\n",
      "0.02869336917456404\n"
     ]
    }
   ],
   "source": [
    "bvv,bpp = backward_induction(env,alpha=1,T=30) \n",
    "bbvv = finite_value_eval(env,1,bpp)\n",
    "print(bvv[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val(0) = 9.967856640248395e-05 at time 1 for finite horizon\n"
     ]
    }
   ],
   "source": [
    "rpp = random_policy(env,2000)\n",
    "rvv = finite_value_eval(env,1,rpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation based\n",
    "evaluation is easy MC estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 3, 2: 3, 3: 3, 4: 3, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 1, 16: 0, 17: 0, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 0, 25: 0, 26: 2, 27: 2, 28: 3, 29: 2, 30: 1, 31: 1, 32: 0, 33: 0, 34: 2, 35: 0, 36: 0, 37: 2, 38: 1, 39: 1, 40: 0, 41: 0, 42: 2, 43: 0, 44: 0, 45: 2, 46: 1, 47: 1, 48: 0, 49: 0, 50: 2, 51: 0, 52: 0, 53: 2, 54: 1, 55: 1, 56: 1, 57: 1, 58: 2, 59: 0, 60: 0, 61: 2, 62: 2, 63: 0}\n"
     ]
    }
   ],
   "source": [
    "print(pp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_eval(env,pol,alpha,T=10**3,nsim = 10**3):\n",
    "    # does MC estimation of the expected reward\n",
    "    running_sum = 0\n",
    "    for _ in range(nsim):\n",
    "        stat = env.reset()\n",
    "        # state = env.reset() #google colab\n",
    "        total_reward = 0\n",
    "        state = stat[0]\n",
    "        for t in range(T): \n",
    "            action = pol[t][state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward +=alpha**t*reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "        running_sum +=total_reward/nsim\n",
    "    return running_sum\n",
    "\n",
    "def infinite_pol_rep(p,T):\n",
    "    return {t: {i:p[i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}\n",
    "\n",
    "def finite_pol_rep(pp,T):\n",
    "    return {t: {i:pp[-t-1][i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC eval = 0.5135162226085791\n",
      "refrence = 0.5165094882368573\n"
     ]
    }
   ],
   "source": [
    "pvv,ppp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) \n",
    "T = 10**3 # basically infinity\n",
    "rep_ppp = infinite_pol_rep(ppp[-1],T)\n",
    "pMC = MC_eval(env,rep_ppp,0.999,T,10**3)\n",
    "print(f\"MC eval = {pMC}\")\n",
    "print(f\"refrence = {pvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC eval = 0.012000000000000004\n",
      "refrence = 0.02869336917456404\n"
     ]
    }
   ],
   "source": [
    "bvv,bpp = backward_induction(env,alpha=1,T=30) \n",
    "T = len(bpp)\n",
    "rep_bpp = finite_pol_rep(bpp,T)\n",
    "bMC = MC_eval(env,rep_bpp,1,T,10**3)\n",
    "print(f\"MC eval = {bMC}\")\n",
    "print(f\"refrence = {bvv[-1][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

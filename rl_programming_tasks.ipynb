{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "from plt_utils import *\n",
    "from test_levels import *\n",
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy computation\n",
    "- policy_update corresponds to $f_v$ <br>\n",
    "- value_update(iterations =1) corresponds to $L_{\\text{policy}}$ <br>\n",
    "- value_update_seidel(iterations =1) uses updated values of v \n",
    "as soon that they are updated like gauss-seidel method. <br>\n",
    "\n",
    "- value_iteration is applying $L_{f_v}v = Uv$ (theorem 1.3.5)  <br>\n",
    "- backward_induction is value_iteration with $v^{T+1} =0$ and time dependence (theorem 1.2.1) <br>\n",
    "- policy_iteration is generalized_iteration because of (theorem 1.3.6) <br>\n",
    "we don't apply $L_{\\pi}$ infinite times but to convergence or $1000$ \n",
    "iterations.\n",
    "\n",
    "the code is obvious, there is a small difference in the formula <br> \n",
    "because rewards are defined differently so you take it in the sum <br>\n",
    "that way you get averaged reward corresponding to our definition <br>\n",
    "and you can decode $b$ with: <br>\n",
    "env.P[state][action] = [(probability, nextstate, reward, done), ...] <br>\n",
    "(To make backward induction time dependent just make env.P[state][action] \n",
    "time dependent.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states(env): return range(env.observation_space.n)\n",
    "def actions(env, state): return env.P[state].keys()\n",
    "# infmetric, norm, only works for v\n",
    "def oometric(v1, v2): return max(abs(v1[i]-v2[i]) for i in v1.keys())\n",
    "\n",
    "def value_update(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*vold[b[1]]) \n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "def value_update_seidel(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(b[0]*(b[2]+alpha*v[b[1]]) # v instead vold, an OG error  for finite problem\n",
    "                           if not(b[3]) else b[0]*b[2] for b in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "\n",
    "def policy_update(env, alpha, v, policy):\n",
    "    for state in states(env):\n",
    "        max_a, max_val = 0, -float(\"inf\") #to select the argmax\n",
    "        for action in actions(env, state):\n",
    "            val = sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in env.P[state][action])\n",
    "            max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "        policy[state] = max_a\n",
    "\n",
    "def value_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        value_update(env,alpha,v,policy,iterations=1)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and oometric(vv[-1], vv[-2]) < eps: break\n",
    "    return vv,pp\n",
    "\n",
    "def backward_induction(env,alpha,T):\n",
    "    return value_iteration(env,alpha,T,0)\n",
    "\n",
    "def generalized_iteration(env,alpha,inner_iter=1,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        value_update(env,alpha,v,policy,inner_iter,eps)\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and pp[-1]==pp[-2]==pp[-3]: break\n",
    "    return vv,pp\n",
    "\n",
    "def policy_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    return generalized_iteration(env=env,alpha=alpha,inner_iter=10**3,max_iter = max_iter,eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 3), (2, 3), (3, 3), (4, 3), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 1), (16, 0), (17, 0), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 0), (25, 0), (26, 2), (27, 2), (28, 3), (29, 2), (30, 1), (31, 1), (32, 0), (33, 0), (34, 2), (35, 0), (36, 0), (37, 2), (38, 1), (39, 1), (40, 0), (41, 0), (42, 2), (43, 0), (44, 0), (45, 2), (46, 1), (47, 1), (48, 0), (49, 0), (50, 2), (51, 0), (52, 0), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 2), (59, 0), (60, 0), (61, 2), (62, 2), (63, 0)]\n",
      "val(0) = 0.5164920595206823 at time 1 for finite horizon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97745f8007b448a489e8de7d6bee25d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=10), IntSlider(value=0, description='ro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "# vv,pp = backward_induction(env,alpha=1,T=1000) #programming task 1\n",
    "vv,pp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) #programming task 2\n",
    "# vv,pp = value_iteration(env,alpha=0.999,max_iter=300,eps = 0.001) #programming task 3\n",
    "sol = list(pp[-1].items()) # asked form of the policy in class\n",
    "print(sol)\n",
    "print(f\"val(0) = {vv[-1][0]} at time 1 for finite horizon\")\n",
    "intvp(vv,pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation\n",
    "- value_eval is for evaluating a policy, basically it is a big  <br>\n",
    "average of the rewards times the probability of getting it <br>\n",
    "lot of terms can be reused, value_update does it basically  <br>\n",
    "- MC_eval just is MC estimation of the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_value_eval(env,alpha,pp):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    vv= []\n",
    "    for p in pp:\n",
    "        value_update(env,alpha,v,p,1,0)\n",
    "        vv.append(v.copy())\n",
    "    print(f\"finite val(0) = {vv[-1][0]}\")\n",
    "    return vv\n",
    "\n",
    "def infinite_value_eval(env,alpha,policy,eps=10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    value_update(env,alpha,v,policy,10**5,eps)\n",
    "    print(f\"infinite val(0) = {v[0]}\")\n",
    "    return v \n",
    "\n",
    "# does MC estimation of the expected reward\n",
    "def MC_eval(env,pol,alpha,T=10**3,nsim = 10**3):\n",
    "    running_sum = 0\n",
    "    for _ in range(nsim):\n",
    "        stat = env.reset()\n",
    "        # state = env.reset() #google colab\n",
    "        total_reward = 0\n",
    "        state = stat[0]\n",
    "        for t in range(T): \n",
    "            action = pol[t][state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += alpha**t*reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "        running_sum +=total_reward/nsim\n",
    "    print(f\"infinite MC val(0) = {running_sum}\")\n",
    "    return running_sum\n",
    "\n",
    "def random_policy(env,T):\n",
    "    return [{state:env.action_space.sample() \n",
    "            for state in states(env)}\n",
    "            for _ in range(T)]\n",
    "\n",
    "def infinite_pol_rep(p,T):\n",
    "    return {t: {i:p[i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}\n",
    "\n",
    "def finite_pol_rep(pp,T):\n",
    "    return {t: {i:pp[-t-1][i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.5164902723354029\n",
      "infinite MC val(0) = 0.5050410388335955\n",
      "reference = 0.5164920595206823\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "alpha = 0.999\n",
    "T = 10**3\n",
    "pvv,ppp = policy_iteration(env,alpha=alpha,max_iter=50,eps=10**(-6)) \n",
    "infinite_value_eval(env,alpha,ppp[-1])\n",
    "MC_eval(env,infinite_pol_rep(ppp[-1],T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {pvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 0.010602851206551666\n",
      "infinite MC val(0) = 0.008758193928672796\n",
      "reference = 0.010602851206551666\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "alpha=0.999\n",
    "T = 30\n",
    "bvv,bpp = backward_induction(env,alpha=alpha,T=T) \n",
    "finite_value_eval(env,alpha=alpha,pp=bpp)\n",
    "MC_eval(env,finite_pol_rep(bpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {bvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 0.3466757182783379\n",
      "infinite MC val(0) = 0.3410222179177072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = trivial()\n",
    "alpha=0.999\n",
    "T = 200\n",
    "rpp = random_policy(env,T)\n",
    "finite_value_eval(env,alpha,rpp)\n",
    "MC_eval(env,finite_pol_rep(rpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-greedy Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume we know the states and the actions ...,\n",
    "we start of from a value function because otherwise \n",
    "sparse rewards are a problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def get_action_from_q(q,state):\n",
    "    i = randint(0,3)\n",
    "    max_a , max_val = i, q[state][i]\n",
    "    for action,val in q[state].items():\n",
    "        max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "    return max_a,max_val    \n",
    "\n",
    "def get_q_from_v(env,v,alpha):\n",
    "    q = {state:{action:0 for action in actions(env,state) } for state in states(env)}\n",
    "    for state in states(env):\n",
    "        for action in actions(env,state): \n",
    "            q[state][action]= sum(b[0]*(b[2]+alpha*v[b[1]]) if not(b[3]) else b[0]*b[2] for b in env.P[state][action]) # uses P\n",
    "    return q\n",
    "\n",
    "def get_policy_from_q(env,q): # Programming task 5 (B)\n",
    "    return {state:get_action_from_q(q,state)[0] for state in states(env)}\n",
    "\n",
    "def Q_learning(env,v,alpha,gamma,T=10**3,nsim = 10**3):\n",
    "    q = get_q_from_v(env,v,alpha) # uses P\n",
    "    for i in range(nsim):\n",
    "        stat = env.reset()\n",
    "        # state = env.reset() #google colab\n",
    "        state = stat[0]\n",
    "        for t in range(T): \n",
    "            action, _ = get_action_from_q(q,state) if randint(0,10)!=0 else (randint(0,3),1)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            _, qval2 = get_action_from_q(q,state)\n",
    "            q[state][action] = (1-gamma)*(q[state][action])+ gamma*(reward+alpha*qval2)\n",
    "            if done: break\n",
    "        env.close()\n",
    "    return q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.0\n",
      "infinite val(0) = 0.2162460422496738\n",
      "This interactive plot gives improvement of the value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e7a5713ba147c0ae5ba88745d8b5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=0), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = time_level()\n",
    "env = envrandom()\n",
    "alpha = 1\n",
    "rp = random_policy(env,1)[0]\n",
    "rv = infinite_value_eval(env,alpha,rp) #uses P\n",
    "q= Q_learning(env,rv,alpha=alpha,gamma=0.2,T=2*10**2,nsim=10**3) # Programming task 5\n",
    "qp = get_policy_from_q(env,q)\n",
    "\n",
    "# Programming task 5 (C) and (D)\n",
    "qv = infinite_value_eval(env,alpha,qp)\n",
    "dv = {state:(qv[state]-rv[state]) for state in states(env)}\n",
    "# intvp([rv], [rp])\n",
    "print(\"This interactive plot gives improvement of the value\")\n",
    "intvp([dv], [qp])\n",
    "# visq(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

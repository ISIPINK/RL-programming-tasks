{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from plt_utils import *\n",
    "from test_levels import *\n",
    "import gym\n",
    "print(gym.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy computation discounted \n",
    "- policy_update corresponds to $f_v$ <br>\n",
    "- value_update(iterations =1) corresponds to $L_{\\text{policy}}$ <br>\n",
    "- value_update_seidel(iterations =1) uses updated values of v \n",
    "as soon that they are updated like gauss-seidel method. <br>\n",
    "\n",
    "- value_iteration is applying $L_{f_v}v = Uv$ (theorem 1.3.5)  <br>\n",
    "- backward_induction is value_iteration with $v^{T+1} =0$ and time dependence (theorem 1.2.1) <br>\n",
    "- policy_iteration is generalized_iteration because of (theorem 1.3.6) <br>\n",
    "we don't apply $L_{\\pi}$ infinite times but to convergence or $1000$ \n",
    "iterations.\n",
    "\n",
    "the code is obvious, there is a small difference in the formula <br> \n",
    "because rewards are defined differently so you take it in the sum <br>\n",
    "that way you get averaged reward corresponding to our definition <br>\n",
    "(To make backward induction time dependent just make env.P[state][action] \n",
    "time dependent.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states(env): return range(env.observation_space.n)\n",
    "def actions(env, state): return env.P[state].keys()\n",
    "# infmetric, norm, only works for v\n",
    "def oometric(v1, v2): return max(abs(v1[i]-v2[i]) for i in v1.keys())\n",
    "\n",
    "def value_update(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(pij*(r+alpha*vold[nextstate]) \n",
    "                           if not(done) else pij*r for (pij,nextstate,r,done) in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "def value_update_seidel(env, alpha, v, policy, iterations=1, eps=10**(-4)):\n",
    "    for _ in range(iterations): \n",
    "        vold = v.copy() \n",
    "        for state in states(env): \n",
    "            v[state] = sum(pij*(r+alpha*v[nextstate]) # v instead vold, an OG error  for finite problem\n",
    "                           if not(done) else pij*r for (pij,nextstate,r,done) in env.P[state][policy[state]])\n",
    "        if oometric(v, vold) < eps: break #convergence\n",
    "\n",
    "\n",
    "def policy_update(env, alpha, v, policy):\n",
    "    for state in states(env):\n",
    "        max_a, max_val = 0, -float(\"inf\") #to select the argmax\n",
    "        for action in actions(env, state):\n",
    "            val = sum(pij*(r+alpha*v[nextstate]) if not(done) else pij*r for (pij,nextstate,r,done) in env.P[state][action])\n",
    "            max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "        policy[state] = max_a\n",
    "\n",
    "def value_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        value_update(env,alpha,v,policy,iterations=1)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and oometric(vv[-1], vv[-2]) < eps: break\n",
    "    return vv,pp\n",
    "\n",
    "def backward_induction(env,alpha,T):\n",
    "    return value_iteration(env,alpha,T,0)\n",
    "\n",
    "def generalized_iteration(env,alpha,inner_iter=1,max_iter = 30,eps = 10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp = [],[] #these are for plotting\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        value_update(env,alpha,v,policy,inner_iter,eps)\n",
    "        policy_update(env,alpha,v,policy)\n",
    "        vv.append(v.copy()) # history of value functions\n",
    "        pp.append(policy.copy()) # policy history\n",
    "        if i>2 and pp[-1]==pp[-2]==pp[-3]: break\n",
    "    return vv,pp\n",
    "\n",
    "def policy_iteration(env,alpha,max_iter = 30,eps = 10**(-6)):\n",
    "    return generalized_iteration(env=env,alpha=alpha,inner_iter=10**3,max_iter = max_iter,eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 3), (2, 3), (3, 3), (4, 3), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 1), (16, 0), (17, 0), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 0), (25, 0), (26, 2), (27, 2), (28, 2), (29, 2), (30, 1), (31, 1), (32, 0), (33, 0), (34, 2), (35, 2), (36, 3), (37, 2), (38, 1), (39, 1), (40, 0), (41, 0), (42, 2), (43, 0), (44, 0), (45, 2), (46, 1), (47, 1), (48, 0), (49, 0), (50, 2), (51, 0), (52, 0), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 0), (60, 0), (61, 2), (62, 2), (63, 0)]\n",
      "val(0) = 0.8334426747830409 at time 1 for finite horizon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6fd3cff1824aa9ac6573ea74410682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=999), IntSlider(value=0, description='r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "# env=gym.make('CliffWalking-v0')\n",
    "# env= gym.make('Taxi-v3') # use value iteration\n",
    "vv,pp = backward_induction(env,alpha=1,T=1000) #programming task 1\n",
    "# vv,pp = policy_iteration(env,alpha=0.999,max_iter=50,eps=10**(-6)) #programming task 2\n",
    "# vv,pp = value_iteration(env,alpha=0.999,max_iter=300,eps = 0.001) #programming task 3\n",
    "sol = list(pp[-1].items()) # asked form of the policy in class\n",
    "print(sol)\n",
    "print(f\"val(0) = {vv[-1][0]} at time 1 for finite horizon\")\n",
    "intvp(vv,pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation discounted\n",
    "- value_eval is for evaluating a policy, basically it is a big  <br>\n",
    "average of the rewards times the probability of getting it <br>\n",
    "lot of terms can be reused, value_update does it basically  <br>\n",
    "- MC_eval just is MC estimation of the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_value_eval(env,alpha,pp):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    vv= []\n",
    "    for p in pp:\n",
    "        value_update(env,alpha,v,p,1,0)\n",
    "        vv.append(v.copy())\n",
    "    print(f\"finite val(0) = {vv[-1][0]}\")\n",
    "    return vv\n",
    "\n",
    "def infinite_value_eval(env,alpha,policy,eps=10**(-6)):\n",
    "    v = {state:0 for state in states(env)} \n",
    "    value_update(env,alpha,v,policy,10**5,eps)\n",
    "    print(f\"infinite val(0) = {v[0]}\")\n",
    "    return v \n",
    "\n",
    "# does MC estimation of the expected reward\n",
    "def MC_eval(env,pol,alpha,T=10**3,nsim = 10**3):\n",
    "    running_sum = 0\n",
    "    for _ in range(nsim):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        for t in range(T): \n",
    "            action = pol[t][state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += alpha**t*reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "        running_sum +=total_reward/nsim\n",
    "    print(f\"infinite MC val(0) = {running_sum}\")\n",
    "    return running_sum\n",
    "\n",
    "def random_policy(env,T):\n",
    "    return [{state:env.action_space.sample() \n",
    "            for state in states(env)}\n",
    "            for _ in range(T)]\n",
    "\n",
    "def infinite_pol_rep(p,T):\n",
    "    return {t: {i:p[i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}\n",
    "\n",
    "def finite_pol_rep(pp,T):\n",
    "    return {t: {i:pp[-t-1][i] \n",
    "              for i in range(env.observation_space.n)}\n",
    "                for t in range(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.7076089300881696\n",
      "infinite MC val(0) = 0.7162973732271237\n",
      "reference = 0.7074435915028664\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "alpha = 0.999\n",
    "T = 10**3\n",
    "pvv,ppp = value_iteration(env,alpha=alpha,max_iter=600,eps=10**(-6)) \n",
    "infinite_value_eval(env,alpha,ppp[-1])\n",
    "MC_eval(env,infinite_pol_rep(ppp[-1],T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {pvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 0.011042662185476242\n",
      "infinite MC val(0) = 0.00974619105238034\n",
      "reference = 0.011042662185476242\n"
     ]
    }
   ],
   "source": [
    "env = time_level()\n",
    "alpha=0.999\n",
    "T = 30\n",
    "bvv,bpp = backward_induction(env,alpha=alpha,T=T) \n",
    "finite_value_eval(env,alpha=alpha,pp=bpp)\n",
    "MC_eval(env,finite_pol_rep(bpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print(f\"reference = {bvv[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite val(0) = 9.163854374067952e-05\n",
      "infinite MC val(0) = 0.0009531108968798942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# env = trivial()\n",
    "env = time_level()\n",
    "alpha=0.999\n",
    "T = 400\n",
    "rpp = random_policy(env,T)\n",
    "finite_value_eval(env,alpha,rpp)\n",
    "MC_eval(env,finite_pol_rep(rpp,T),alpha=alpha,T=T,nsim=10**3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation average\n",
    "We use theorem 1.4.14 to calculate $P^*$ as \n",
    "$$\n",
    "P^* = \\lim_{n \\to \\infty} (0.5 P + 0.5I)^{n} .\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_array,eye\n",
    "from scipy.sparse.linalg import norm \n",
    "\n",
    "def P_from_policy(env,policy):\n",
    "    n = len(states(env))\n",
    "    P = dok_array((n, n), dtype=np.float32)\n",
    "    for i in states(env):\n",
    "        for (pij,j,_,_) in env.P[i][policy[i]]:\n",
    "            P[i,j] += pij \n",
    "    return P\n",
    "\n",
    "def power_convergence(P,max_iter,eps =10**(-12)):\n",
    "    Q=P\n",
    "    for _ in range(max_iter):\n",
    "        Qnew = Q@Q\n",
    "        if norm(Q-Qnew)<eps:break\n",
    "        Q = Qnew\n",
    "    return Q \n",
    "\n",
    "def treshold(A,eps = 10**(-10)):\n",
    "    A.data[abs(A.data) < eps] = 0\n",
    "    A.eliminate_zeros()\n",
    "    return A\n",
    "\n",
    "def stationary_matrix(P, eps = 10**(-10)):\n",
    "    Pstar = power_convergence(0.9*P+0.1*eye(P.shape[0]),15,eps)\n",
    "    return treshold(Pstar,eps) \n",
    "\n",
    "def infinite_value_eval_average(env,policy,rewd):\n",
    "    P = P_from_policy(env,policy)\n",
    "    r = np.array([rewd[state][policy[state]] for state in states(env)])\n",
    "    Q = stationary_matrix(P) \n",
    "    phi = Q@r\n",
    "    return {state:phi[state] for state in states(env)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.8333661771717522\n",
      "ind0: P, ind1: Pstar, ind2: Zinv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809a6930aec34bdcbb3c0b68aa9a260b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=2), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = time_level()\n",
    "_,ppp = policy_iteration(env,alpha=1) \n",
    "pvv = [infinite_value_eval(env,1,ppp[-1])]\n",
    "P = P_from_policy(env,ppp[-1])\n",
    "Pstar = stationary_matrix(P)\n",
    "print(f\"ind0: P, ind1: Pstar, ind2: Zinv\")\n",
    "intPP([P]+[Pstar]+[eye(Pstar.shape[0])-P+Pstar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the reward vector is $1$ at the goal, the average reward must correspond to \n",
    "the discounted reward with $\\alpha =1$ which in this case can be calculated with \n",
    "the discounted tools. Here we check that they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between average_pvv and pvv[-1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8fbfda757a466ea84b420c8ba8bc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=0), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewd = {state:{action:0 if state != 63 else 1 for action in actions(env,state)} for state in states(env)} \n",
    "average_pvv = infinite_value_eval_average(env,ppp[-1],rewd) \n",
    "df ={state:average_pvv[state]-pvv[-1][state] for state in states(env)}\n",
    "print(\"difference between average_pvv and pvv[-1]\")\n",
    "intvp([df],[ppp[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy computation average\n",
    "This is Algorithm 1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import bicgstab\n",
    "\n",
    "def better_average_actions(env,rewd,phi,u0,policy,eps = 10**(-4)):\n",
    "    sol = {state:[] for state in states(env)}\n",
    "    done = True\n",
    "    for state in states(env): \n",
    "        for action in actions(env,state):\n",
    "            if policy[state] == action: continue\n",
    "            phiaction = sum(pij*phi[nextstate] for (pij,nextstate,_,_) in env.P[state][action])\n",
    "            if phiaction> phi[state]+eps:\n",
    "                sol[state].append(action)\n",
    "                done = False\n",
    "                continue\n",
    "            if abs(phiaction - phi[state])<eps:\n",
    "                LHS14 = rewd[state][action]+sum(pij*u0[nextstate] for (pij,nextstate,_,_) in env.P[state][action])\n",
    "                if LHS14-phi[state]-u0[state]>eps:\n",
    "                    sol[state].append(action)\n",
    "                    done =False\n",
    "    return sol,done\n",
    "\n",
    "def policy_update_average(env,policy,B):\n",
    "    for state in states(env):\n",
    "        if len(B[state])!=0: \n",
    "            if policy[state] !=B[state][0]: \n",
    "                policy[state] = B[state][0]\n",
    "            elif len(B[state])>=2:\n",
    "                policy[state]= B[state][1]\n",
    "\n",
    "\n",
    "\n",
    "def policy_iteration_average(env,rewd,max_iter = 30):\n",
    "    policy = {state:0 for state in states(env)} \n",
    "    vv,pp,u0 = [],[],np.zeros(env.observation_space.n)\n",
    "    I = eye(env.observation_space.n)\n",
    "    for i in range(max_iter):\n",
    "        r = np.array([rewd[state][policy[state]] for state in states(env)])\n",
    "        P = P_from_policy(env,policy)\n",
    "        Pstar = stationary_matrix(P)\n",
    "        Zinv = I-P+Pstar\n",
    "        phi =  Pstar@r \n",
    "        u0, _ = bicgstab(Zinv,r-Zinv@phi,x0=u0)\n",
    "        B,done = better_average_actions(env,rewd,phi,u0,policy)\n",
    "        vv.append({state:phi[state] for state in states(env)})\n",
    "        pp.append(policy.copy())\n",
    "        if done or pp[-1] in pp[-3:-1]: break\n",
    "        policy_update_average(env,policy,B)\n",
    "    return vv,pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb8d1ef58754a27a5a24b2affc8f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=11), IntSlider(value=0, description='ro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = time_level()\n",
    "# env = envrandom()\n",
    "rewd = {state:{action:0 if state != 63 else 1 for action in actions(env,state)} for state in states(env)} \n",
    "pvv, ppp = policy_iteration(env,alpha=1)\n",
    "apvv, appp = policy_iteration_average(env,rewd,max_iter=30)\n",
    "intvp(apvv,appp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it0: apvv[-1] vs pvv[-1], it1: apvv[-1] vs random policy\n",
      "the comparison is the difference\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7ef037534f4c4aadc50d6a48f65d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=1), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_average_vv = infinite_value_eval_average(env,random_policy(env,1)[0],rewd) # programming task 4.3\n",
    "df1 ={state:apvv[-1][state]-pvv[-1][state] for state in states(env)} \n",
    "df2 ={state:apvv[-1][state]-random_average_vv[state] for state in states(env)} \n",
    "print(\"it0: apvv[-1] vs pvv[-1], it1: apvv[-1] vs random policy\")\n",
    "print(\"the comparison is the difference\")\n",
    "intvp([df1,df2],2*[appp[-1]]) # programming task 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-greedy Q-learning\n",
    "We assume we know the states and the actions ...,\n",
    "we start of from a value function because otherwise \n",
    "sparse rewards are a problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def get_action_from_q(q,state):\n",
    "    i = randint(0,3)\n",
    "    max_a , max_val = i, q[state][i]\n",
    "    for action,val in q[state].items():\n",
    "        max_a, max_val = (action, val) if val > max_val else (max_a, max_val)\n",
    "    return max_a,max_val\n",
    "\n",
    "def get_q_from_v(env,v,alpha):\n",
    "    return {state:{action:\n",
    "      sum(pij*(r+alpha*v[nextstate]) if not(done) else pij*r\n",
    "          for (pij,nextstate,r,done) in env.P[state][action]) for action in actions(env,state) } for state in states(env)}\n",
    "\n",
    "def get_policy_from_q(env,q): # Programming task 5 (B)\n",
    "    return {state:get_action_from_q(q,state)[0] for state in states(env)}\n",
    "\n",
    "def val_est(state):\n",
    "    row,col = divmod(state,8)\n",
    "    return (row+col)/14\n",
    "\n",
    "def q_0(env):\n",
    "    return {state:{action:0 for action in actions(env,state) } for state in states(env)}\n",
    "\n",
    "def q_est(env):\n",
    "    return {state:{action:val_est(state) for action in actions(env,state) } for state in states(env)}\n",
    "\n",
    "def Q_learning(env,v,alpha,gamma,T=10**3,nsim = 10**3):\n",
    "    goal_count = 0\n",
    "    q = get_q_from_v(env,v,alpha) # uses P\n",
    "    # q = q_est(env)\n",
    "    # q = q_0(env)\n",
    "    for i in range(nsim):\n",
    "        state = env.reset()[0] \n",
    "        for t in range(T):\n",
    "            action, _ = get_action_from_q(q,state) if randint(0,30)!=0 else (randint(0,3),1)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            _, qval2 = get_action_from_q(q,state)\n",
    "            q[state][action] = (1-gamma)*(q[state][action])+ gamma*(reward+alpha*qval2)\n",
    "            goal_count +=reward\n",
    "            if done: break\n",
    "        env.close()\n",
    "    print(f\"reached goal {goal_count} times in sims\")\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite val(0) = 0.0\n",
      "reached goal 434.0 times in sims\n",
      "infinite val(0) = 0.10272103289778609\n",
      "it2: improvement of the value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b53c7c779534336875733680f32e2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iterations', max=1), IntSlider(value=0, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = time_level()\n",
    "env = envrandom()\n",
    "alpha = 1\n",
    "rp = random_policy(env,1)[0]\n",
    "rv = infinite_value_eval(env,alpha,rp) #uses P\n",
    "q= Q_learning(env,rv,alpha=alpha,gamma=0.2,T=2*10**2,nsim=10**4) # Programming task 5\n",
    "qp = get_policy_from_q(env,q)\n",
    "\n",
    "# Programming task 5 (C) and (D)\n",
    "qv = infinite_value_eval(env,alpha,qp)\n",
    "dv = {state:(qv[state]-rv[state]) for state in states(env)}\n",
    "# intvp([rv], [rp])\n",
    "print(\"it2: improvement of the value\")\n",
    "intvp([qv,dv], 2*[qp])\n",
    "# visq(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
